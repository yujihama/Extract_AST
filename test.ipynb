{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Document Analysis Agent\n",
                "\n",
                "This notebook implements a LangChain v1.x agent (tool-calling, LangGraph-based) that autonomously analyzes a text file and extracts its structure as an AST."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 準備"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 129,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install -q \"langchain>=1.0,<2\" \"langchain-core>=1.0,<2\" \"langchain-openai\" \"langgraph\" \"pydantic>=2,<3\" \"python-dotenv\" \"pypdf>=3.0.0\" \"pymupdf>=1.23.0\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "\n",
                "import dotenv\n",
                "\n",
                "# .env が存在する場合、環境変数を読み込む\n",
                "dotenv.load_dotenv()\n",
                "\n",
                "from typing import List, Optional\n",
                "from pydantic import BaseModel, Field, field_validator\n",
                "\n",
                "from langchain_core.tools import tool\n",
                "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
                "from langchain.agents import create_agent\n",
                "from langchain.agents.middleware import SummarizationMiddleware, TodoListMiddleware, ContextEditingMiddleware, ClearToolUsesEdit\n",
                "\n",
                "def build_llm():\n",
                "    \"\"\"環境変数から OpenAI / Azure OpenAI のチャットモデルを作成する。\"\"\"\n",
                "    provider = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()\n",
                "    model = \"gpt-5.2\"\n",
                "    temperature = float(os.getenv(\"TEMPERATURE\") or \"0\")\n",
                "\n",
                "    if provider in {\"azure\", \"azureopenai\", \"azure_openai\"}:\n",
                "        return AzureChatOpenAI(\n",
                "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
                "            azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
                "            or os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
                "            or model,\n",
                "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
                "            or os.getenv(\"OPENAI_API_VERSION\"),\n",
                "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
                "            temperature=temperature,\n",
                "        )\n",
                "\n",
                "    return ChatOpenAI(\n",
                "        model=model,\n",
                "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
                "        temperature=temperature,\n",
                "    )\n",
                "\n",
                "\n",
                "# 注意:\n",
                "# - OpenAI: OPENAI_API_KEY を設定（必要なら OPENAI_MODEL / MODEL も）\n",
                "# - Azure OpenAI: LLM_PROVIDER=azure と AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_API_KEY /\n",
                "#   AZURE_OPENAI_API_VERSION（または OPENAI_API_VERSION）/ AZURE_OPENAI_DEPLOYMENT_NAME を設定"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. カスタムツールの設定"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 122,
            "metadata": {},
            "outputs": [],
            "source": [
                "@tool\n",
                "def read_text_segment(file_path: str, start: int, length: int, intent: str) -> str:\n",
                "    \"\"\"\n",
                "    テキストファイルの特定のセグメントを読み取る。\n",
                "    \n",
                "    引数:\n",
                "        file_path: テキストファイルのパス。\n",
                "        start: 開始文字インデックス（0ベース）。\n",
                "        length: 読み取る文字数。\n",
                "        intent: ツール呼び出しの意図。\n",
                "\n",
                "    戻り値:\n",
                "        'start' から指定された 'length' の部分文字列。\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # 注意: \"start\" は文字インデックスであり、バイトオフセットではない。\n",
                "        # テキストモードでは、seek(start) を使用すると UTF-8 のマルチバイトシーケンスの途中に着地する可能性がある。\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            if start > 0:\n",
                "                f.read(start)\n",
                "            return f.read(length)\n",
                "    except Exception as e:\n",
                "        return f\"Error reading file: {str(e)}\"\n",
                "\n",
                "@tool\n",
                "def extract_regex_matches(\n",
                "    file_path: str,\n",
                "    regex_pattern: str,\n",
                "    intent: str,\n",
                "    regex_patterns: Optional[List[str]] = None,\n",
                "    max_matches: int = 50,\n",
                "    offset_matches: int = 0,\n",
                "    max_output_chars: int = 8000,\n",
                "    count_only: bool = False,\n",
                "    dedupe: bool = False,\n",
                "    save_to: Optional[str] = None,\n",
                "    include_line_text: bool = True,\n",
                ") -> str:\n",
                "    \"\"\"\n",
                "    ファイルから指定された正規表現パターンに一致するテキストを抽出する。\n",
                "\n",
                "    - `regex_patterns` を指定すると、複数正規表現で検索して「行番号＋行テキスト＋マッチ文字列」を返す。\n",
                "    - `regex_patterns` が未指定の場合は、従来どおり `regex_pattern`（単一）で検索する。\n",
                "\n",
                "    引数:\n",
                "        file_path: テキストファイルのパス。\n",
                "        regex_pattern: 検索する Python スタイルの正規表現パターン（後方互換用。`regex_patterns` 指定時は無視される）。\n",
                "        intent: ツール呼び出しの意図。\n",
                "        regex_patterns: 検索する正規表現パターンのリスト（複数指定用）。\n",
                "        max_matches: 返却する一致（サンプル）の最大件数（デフォルト: 50）。\n",
                "        offset_matches: 先頭からスキップする一致件数（ページング用途）。\n",
                "        max_output_chars: 返却文字列の上限（ベストエフォート）。超える場合はサンプルを削って短縮する。\n",
                "        count_only: True の場合、件数のみ返す（サンプルは返さない）。\n",
                "        dedupe: True の場合、サンプルを重複排除して返す（返却件数は max_matches まで）。\n",
                "        save_to: 指定した場合、offset_matches 以降の一致を JSONL でファイルへ保存する（コンテキスト節約用）。\n",
                "        include_line_text: True の場合、ヒットした行テキストも返す。\n",
                "\n",
                "    戻り値:\n",
                "        JSON 文字列（件数、サンプル、トランケーション有無、保存先など）。\n",
                "\n",
                "    返却される `sample` の要素は以下の形:\n",
                "        {\n",
                "          \"pattern_index\": 0,\n",
                "          \"pattern\": \"...\",\n",
                "          \"line_number\": 123,\n",
                "          \"match\": \"...\" | [\"...\", ...],\n",
                "          \"full_match\": \"...\",\n",
                "          \"line\": \"...\"  # include_line_text=true の場合\n",
                "        }\n",
                "\n",
                "    注意:\n",
                "        このツールは「行番号」を返すため、検索は基本的に行単位で行う（複数行にまたがる正規表現は想定外）。\n",
                "    \"\"\"\n",
                "    try:\n",
                "        import json\n",
                "\n",
                "        if max_matches < 0:\n",
                "            max_matches = 0\n",
                "        if offset_matches < 0:\n",
                "            offset_matches = 0\n",
                "        if max_output_chars is None or max_output_chars <= 0:\n",
                "            max_output_chars = 8000\n",
                "\n",
                "        patterns_in = regex_patterns if (regex_patterns is not None and len(regex_patterns) > 0) else [regex_pattern]\n",
                "        # 空文字は除外（`regex_patterns` を使う場合に `regex_pattern` をダミーで渡してもOKにする）\n",
                "        patterns_in = [p for p in patterns_in if isinstance(p, str) and p.strip()]\n",
                "        if not patterns_in:\n",
                "            return json.dumps(\n",
                "                {\n",
                "                    \"ok\": False,\n",
                "                    \"error\": \"regex_pattern または regex_patterns に少なくとも1つの正規表現を指定してください。\",\n",
                "                },\n",
                "                ensure_ascii=False,\n",
                "            )\n",
                "\n",
                "        compiled = []\n",
                "        for i, p in enumerate(patterns_in):\n",
                "            try:\n",
                "                compiled.append((i, p, re.compile(p)))\n",
                "            except Exception as e:\n",
                "                return json.dumps(\n",
                "                    {\n",
                "                        \"ok\": False,\n",
                "                        \"error\": f\"正規表現のコンパイルに失敗しました (index={i}): {str(e)}\",\n",
                "                        \"pattern\": p,\n",
                "                    },\n",
                "                    ensure_ascii=False,\n",
                "                )\n",
                "\n",
                "        def _match_to_item(m: re.Match, group_count: int):\n",
                "            # re.findall と同等の返却形（グループなし: 全体、グループ1つ: そのグループ、複数: タプル）\n",
                "            if group_count == 0:\n",
                "                return m.group(0)\n",
                "            if group_count == 1:\n",
                "                return m.group(1)\n",
                "            return m.groups()\n",
                "\n",
                "        # save_to が指定されている場合は JSONL で全件（offset 以降）を保存\n",
                "        save_f = None\n",
                "        if save_to:\n",
                "            save_dir = os.path.dirname(os.path.abspath(save_to))\n",
                "            if save_dir:\n",
                "                os.makedirs(save_dir, exist_ok=True)\n",
                "            save_f = open(save_to, \"w\", encoding=\"utf-8\", newline=\"\\n\")\n",
                "\n",
                "        total = 0\n",
                "        total_after_offset = 0\n",
                "        sample = []\n",
                "        seen = set()\n",
                "\n",
                "        try:\n",
                "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                for line_number, line in enumerate(f, start=1):\n",
                "                    line_no_nl = line.rstrip(\"\\n\")\n",
                "                    for pattern_index, pattern_text, pattern_obj in compiled:\n",
                "                        group_count = pattern_obj.groups\n",
                "                        for m in pattern_obj.finditer(line_no_nl):\n",
                "                            total += 1\n",
                "                            if total <= offset_matches:\n",
                "                                continue\n",
                "                            total_after_offset += 1\n",
                "\n",
                "                            item = _match_to_item(m, group_count)\n",
                "\n",
                "                            record = {\n",
                "                                \"pattern_index\": pattern_index,\n",
                "                                \"pattern\": pattern_text,\n",
                "                                \"line_number\": line_number,\n",
                "                                \"match\": (list(item) if isinstance(item, tuple) else item),\n",
                "                                \"full_match\": m.group(0),\n",
                "                            }\n",
                "                            if include_line_text:\n",
                "                                record[\"line\"] = line_no_nl\n",
                "\n",
                "                            if save_f is not None:\n",
                "                                save_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
                "\n",
                "                            if count_only:\n",
                "                                continue\n",
                "\n",
                "                            if len(sample) >= max_matches:\n",
                "                                continue\n",
                "\n",
                "                            if dedupe:\n",
                "                                key = repr(record)\n",
                "                                if key in seen:\n",
                "                                    continue\n",
                "                                seen.add(key)\n",
                "\n",
                "                            sample.append(record)\n",
                "        finally:\n",
                "            if save_f is not None:\n",
                "                save_f.close()\n",
                "\n",
                "        payload = {\n",
                "            \"ok\": True,\n",
                "            \"file_path\": file_path,\n",
                "            \"regex_pattern\": regex_pattern,\n",
                "            \"regex_patterns\": patterns_in,\n",
                "            \"total_matches\": total,\n",
                "            \"offset_matches\": offset_matches,\n",
                "            \"matches_after_offset\": total_after_offset,\n",
                "            \"returned_matches\": 0 if count_only else len(sample),\n",
                "            \"truncated\": (False if count_only else (total_after_offset > len(sample))),\n",
                "            \"count_only\": bool(count_only),\n",
                "            \"dedupe\": bool(dedupe),\n",
                "            \"saved_to\": save_to,\n",
                "            \"include_line_text\": bool(include_line_text),\n",
                "            \"sample\": [] if count_only else sample,\n",
                "            \"hint\": \"結果が多い場合は count_only=true で件数確認→ offset_matches でページング。全件が必要なら save_to を指定してください。\",\n",
                "        }\n",
                "\n",
                "        # 出力サイズが大きい場合は sample を削って短縮（ベストエフォート）\n",
                "        def _dump(p: dict) -> str:\n",
                "            return json.dumps(p, ensure_ascii=False)\n",
                "\n",
                "        out = _dump(payload)\n",
                "        if not count_only and len(out) > max_output_chars:\n",
                "            while payload[\"sample\"] and len(out) > max_output_chars:\n",
                "                payload[\"sample\"].pop()\n",
                "                payload[\"returned_matches\"] = len(payload[\"sample\"])\n",
                "                payload[\"truncated\"] = True\n",
                "                out = _dump(payload)\n",
                "            if len(out) > max_output_chars:\n",
                "                payload[\"sample\"] = []\n",
                "                payload[\"returned_matches\"] = 0\n",
                "                payload[\"truncated\"] = True\n",
                "                payload[\"hint\"] = (\n",
                "                    \"出力が大きすぎるためサンプルは省略しました。count_only=true で件数確認し、\"\n",
                "                    \"必要なら save_to でファイル出力してください。\"\n",
                "                )\n",
                "                out = _dump(payload)\n",
                "\n",
                "        return out\n",
                "    except Exception as e:\n",
                "        return f\"Error extracting matches: {str(e)}\"\n",
                "\n",
                "@tool\n",
                "def get_file_length(file_path: str, intent: str) -> str:\n",
                "    \"\"\"\n",
                "    ファイルの総文字数を返す。\n",
                "    \n",
                "    引数:\n",
                "        file_path: テキストファイルのパス。\n",
                "        intent: ツール呼び出しの意図。\n",
                "\n",
                "    戻り値:\n",
                "        ファイルの総文字数。\n",
                "    \"\"\"\n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            return str(len(f.read()))\n",
                "    except Exception as e:\n",
                "        return f\"Error getting file length: {str(e)}\"\n",
                "\n",
                "class ReadTextFileArgs(BaseModel):\n",
                "    \"\"\"read_text_file ツールの引数。\"\"\"\n",
                "    file_path: str = Field(..., description=\"テキストファイルのパス。\")\n",
                "    start: Optional[int] = Field(None, description=\"開始文字インデックス（0ベース）。このパラメータを省略すると、先頭から読み取る。\")\n",
                "    length: Optional[int] = Field(None, description=\"読み取る文字数。このパラメータを省略すると、start 位置から 100 文字読み取る（start も省略されている場合は先頭から 100 文字）。\")\n",
                "    intent: str = Field(..., description=\"ツール呼び出しの意図。\")\n",
                "\n",
                "@tool(args_schema=ReadTextFileArgs)\n",
                "def read_text_file(file_path: str, start: Optional[int] = None, length: Optional[int] = None, intent: str = \"\") -> str:\n",
                "    \"\"\"\n",
                "    UTF-8 としてテキストファイルを読み取る。ファイルの特定のセグメントを読み取ることができる。\n",
                "    \n",
                "    デフォルト動作: start と length の両方を省略した場合、最初の 100 文字を読み取る。\n",
                "    これにより、大きなファイルを読み取る際にコンテキストウィンドウの制限を超えることを防ぐ。\n",
                "    \n",
                "    例:\n",
                "    - read_text_file(\"file.txt\") -> 最初の 100 文字を読み取る\n",
                "    - read_text_file(\"file.txt\", start=0, length=1000) -> 最初の 1000 文字を読み取る\n",
                "    - read_text_file(\"file.txt\", start=500, length=2000) -> 位置 500 から 2000 文字を読み取る\n",
                "    - read_text_file(\"file.txt\", start=1000) -> 位置 1000 から 100 文字を読み取る\n",
                "    \n",
                "    引数:\n",
                "        file_path: テキストファイルのパス。\n",
                "        start: 開始文字インデックス（0ベース）。省略すると先頭から読み取る。\n",
                "        length: 読み取る文字数。省略すると start 位置から 100 文字を読み取る。\n",
                "        intent: ツール呼び出しの意図。\n",
                "    戻り値:\n",
                "        ファイルコンテンツのセグメント（デフォルト: start と length の両方を省略した場合は最初の 100 文字）。\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # 注意: \"start\" は文字インデックスであり、バイトオフセットではない。\n",
                "        # テキストモードでは、seek(start) を使用すると UTF-8 のマルチバイトシーケンスの途中に着地する可能性がある。\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            if start is not None and start > 0:\n",
                "                f.read(start)\n",
                "            if length is not None:\n",
                "                return f.read(length)\n",
                "            # デフォルト: 100 文字を読み取る\n",
                "            return f.read(100)\n",
                "    except Exception as e:\n",
                "        return f\"Error reading file: {str(e)}\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### ast_store"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 120,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 永続化 AST ストアツール（ノートブック内で自己完結） ---\n",
                "import json\n",
                "import re\n",
                "from dataclasses import dataclass\n",
                "from datetime import datetime, timedelta, timezone\n",
                "from typing import Any, Dict, List, Literal, Optional, Tuple\n",
                "from uuid import uuid4\n",
                "\n",
                "\n",
                "def _utc_now_iso() -> str:\n",
                "    return datetime.now(timezone.utc).isoformat()\n",
                "\n",
                "\n",
                "def _ensure_parent_dir(path: str) -> None:\n",
                "    parent = os.path.dirname(os.path.abspath(path))\n",
                "    if parent:\n",
                "        os.makedirs(parent, exist_ok=True)\n",
                "\n",
                "\n",
                "def _atomic_write_text(path: str, text: str, encoding: str = \"utf-8\") -> None:\n",
                "    \"\"\"os.replace を使用してファイルをアトミックに書き込む（ベストエフォート）。\"\"\"\n",
                "    _ensure_parent_dir(path)\n",
                "    tmp_path = f\"{path}.tmp\"\n",
                "    with open(tmp_path, \"w\", encoding=encoding, newline=\"\\n\") as f:\n",
                "        f.write(text)\n",
                "    os.replace(tmp_path, path)\n",
                "\n",
                "\n",
                "def _load_json(path: str) -> Dict[str, Any]:\n",
                "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "\n",
                "def _dump_json(data: Any) -> str:\n",
                "    return json.dumps(data, ensure_ascii=False, indent=2)\n",
                "\n",
                "\n",
                "def _get_meta(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
                "    meta = ast.get(\"__meta__\")\n",
                "    if isinstance(meta, dict):\n",
                "        if \"rev\" not in meta:\n",
                "            meta[\"rev\"] = 0\n",
                "        return meta\n",
                "    meta = {\"rev\": 0, \"updated_at\": None}\n",
                "    ast[\"__meta__\"] = meta\n",
                "    return meta\n",
                "\n",
                "\n",
                "def _bump_meta(ast: Dict[str, Any]) -> Dict[str, Any]:\n",
                "    meta = _get_meta(ast)\n",
                "    meta[\"rev\"] = int(meta.get(\"rev\") or 0) + 1\n",
                "    meta[\"updated_at\"] = _utc_now_iso()\n",
                "    return meta\n",
                "\n",
                "\n",
                "# load_metaで発行するワンタイム編集トークン（Notebookカーネル内メモリに保持）\n",
                "# token -> {ast_path, scope_kind, scope_value, issued_rev, expires_at}\n",
                "_EDIT_TOKENS: Dict[str, Dict[str, Any]] = {}\n",
                "_EDIT_TOKEN_TTL = timedelta(minutes=10)\n",
                "\n",
                "\n",
                "def _issue_edit_token(*, ast_path: str, scope_kind: str, scope_value: Any, issued_rev: int) -> str:\n",
                "    token = uuid4().hex\n",
                "    _EDIT_TOKENS[token] = {\n",
                "        \"ast_path\": os.path.abspath(ast_path),\n",
                "        \"scope_kind\": scope_kind,\n",
                "        \"scope_value\": scope_value,\n",
                "        \"issued_rev\": int(issued_rev),\n",
                "        \"expires_at\": datetime.now(timezone.utc) + _EDIT_TOKEN_TTL,\n",
                "    }\n",
                "    return token\n",
                "\n",
                "\n",
                "def _consume_edit_token(*, token: str, ast_path: str, scope_kind: str, scope_value: Any, current_rev: int) -> Optional[str]:\n",
                "    rec = _EDIT_TOKENS.get(token)\n",
                "    if not rec:\n",
                "        return \"edit_token が欠落しているか無効です。新しいトークンを取得するために ast_store(action='load_meta', ...) を呼び出してください。\"\n",
                "\n",
                "    if rec.get(\"expires_at\") and datetime.now(timezone.utc) > rec[\"expires_at\"]:\n",
                "        _EDIT_TOKENS.pop(token, None)\n",
                "        return \"edit_token の有効期限が切れました。load_meta を再度呼び出してください。\"\n",
                "\n",
                "    if rec.get(\"ast_path\") != os.path.abspath(ast_path):\n",
                "        return \"edit_token が ast_path と一致しません。load_meta を再度呼び出してください。\"\n",
                "\n",
                "    issued_rev = rec.get(\"issued_rev\")\n",
                "    if issued_rev is None or int(issued_rev) != int(current_rev):\n",
                "        return f\"古いトークンです（issued_rev={issued_rev}, current_rev={current_rev}）。load_meta を再度呼び出してください。\"\n",
                "\n",
                "    if rec.get(\"scope_kind\") != scope_kind:\n",
                "        return \"edit_token のスコープが一致しません。load_meta を再度呼び出してください。\"\n",
                "\n",
                "    if rec.get(\"scope_value\") != scope_value:\n",
                "        return \"edit_token のスコープ値が一致しません。load_meta を再度呼び出してください。\"\n",
                "\n",
                "    # 消費\n",
                "    _EDIT_TOKENS.pop(token, None)\n",
                "    return None\n",
                "\n",
                "\n",
                "def _normalize_path_indices(path_indices: Optional[List[int]]) -> List[int]:\n",
                "    if path_indices is None:\n",
                "        return []\n",
                "    return list(path_indices)\n",
                "\n",
                "\n",
                "@dataclass(frozen=True)\n",
                "class _NodeRef:\n",
                "    node: Dict[str, Any]\n",
                "    parent: Optional[Dict[str, Any]]\n",
                "    index_in_parent: Optional[int]\n",
                "\n",
                "\n",
                "def _get_children_list(node: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
                "    children = node.get(\"children\")\n",
                "    if children is None:\n",
                "        children = []\n",
                "        node[\"children\"] = children\n",
                "    if not isinstance(children, list):\n",
                "        raise ValueError(\"無効な AST: 'children' はリストである必要があります。\")\n",
                "    return children  # type: ignore[return-value]\n",
                "\n",
                "\n",
                "def _traverse(ast: Dict[str, Any], node_path: List[int]) -> _NodeRef:\n",
                "    \"\"\"node_path: [] = ルート、[0] = 最初の子、[0,2] = 最初の子の3番目の子。\"\"\"\n",
                "    if \"root\" not in ast or not isinstance(ast[\"root\"], dict):\n",
                "        raise ValueError(\"無効な AST: 'root' オブジェクトが欠落しています。\")\n",
                "\n",
                "    current = ast[\"root\"]\n",
                "    parent: Optional[Dict[str, Any]] = None\n",
                "    idx_in_parent: Optional[int] = None\n",
                "\n",
                "    for idx in node_path:\n",
                "        children = _get_children_list(current)\n",
                "        if idx < 0 or idx >= len(children):\n",
                "            raise IndexError(f\"無効なパスインデックス {idx}。子要素の長さは {len(children)} です。\")\n",
                "        parent = current\n",
                "        idx_in_parent = idx\n",
                "        current = children[idx]\n",
                "        if not isinstance(current, dict):\n",
                "            raise ValueError(\"無効な AST: ノードはオブジェクトである必要があります。\")\n",
                "\n",
                "    return _NodeRef(node=current, parent=parent, index_in_parent=idx_in_parent)\n",
                "\n",
                "\n",
                "def _make_node(section_title: Optional[str], content_summary: str) -> Dict[str, Any]:\n",
                "    return {\n",
                "        \"section_title\": section_title,\n",
                "        \"content_summary\": content_summary,\n",
                "        \"children\": [],\n",
                "    }\n",
                "\n",
                "\n",
                "def _normalize_title(title: Optional[str]) -> str:\n",
                "    if title is None:\n",
                "        return \"\"\n",
                "    t = str(title).replace(\"\\u3000\", \" \")\n",
                "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
                "    return t\n",
                "\n",
                "\n",
                "def _titles_equal(a: Optional[str], b: Optional[str]) -> bool:\n",
                "    return _normalize_title(a) == _normalize_title(b)\n",
                "\n",
                "\n",
                "def _ensure_titles_path(\n",
                "    ast: Dict[str, Any],\n",
                "    titles: List[str],\n",
                "    *,\n",
                "    create_missing: bool,\n",
                "    created_default_summary: str = \"\",\n",
                ") -> Tuple[List[int], bool]:\n",
                "    \"\"\"セクションタイトルのリスト（パス）によってノードを解決する。戻り値: (node_path, created_any)。\"\"\"\n",
                "    if \"root\" not in ast or not isinstance(ast[\"root\"], dict):\n",
                "        raise ValueError(\"無効な AST: 'root' オブジェクトが欠落しています。\")\n",
                "\n",
                "    current = ast[\"root\"]\n",
                "    path: List[int] = []\n",
                "    created_any = False\n",
                "\n",
                "    titles_norm: List[str] = [str(t) for t in titles if str(t).strip() != \"\"]\n",
                "    if titles_norm:\n",
                "        root_title = current.get(\"section_title\")\n",
                "        if _titles_equal(root_title, titles_norm[0]):\n",
                "            titles_norm = titles_norm[1:]\n",
                "\n",
                "    for raw_title in titles_norm:\n",
                "        title = str(raw_title)\n",
                "        target_norm = _normalize_title(title)\n",
                "\n",
                "        children = _get_children_list(current)\n",
                "        matches = [\n",
                "            i\n",
                "            for i, child in enumerate(children)\n",
                "            if isinstance(child, dict) and _normalize_title(child.get(\"section_title\")) == target_norm\n",
                "        ]\n",
                "\n",
                "        if not matches:\n",
                "            if not create_missing:\n",
                "                raise ValueError(f\"タイトル '{title}' のノードが見つかりません。\")\n",
                "            children.append(_make_node(title, created_default_summary))\n",
                "            idx = len(children) - 1\n",
                "            created_any = True\n",
                "        else:\n",
                "            # 同じ親の下に重複が存在する場合、決定論的に最初の一致を選択する。\n",
                "            idx = matches[0]\n",
                "\n",
                "        path.append(idx)\n",
                "        current = children[idx]\n",
                "        if not isinstance(current, dict):\n",
                "            raise ValueError(\"無効な AST: ノードはオブジェクトである必要があります。\")\n",
                "\n",
                "    return path, created_any\n",
                "\n",
                "\n",
                "def _titles_for_path(ast: Dict[str, Any], node_path: List[int]) -> List[str]:\n",
                "    if \"root\" not in ast or not isinstance(ast[\"root\"], dict):\n",
                "        raise ValueError(\"無効な AST: 'root' オブジェクトが欠落しています。\")\n",
                "    current = ast[\"root\"]\n",
                "    titles: List[str] = []\n",
                "    for idx in node_path:\n",
                "        children = _get_children_list(current)\n",
                "        if idx < 0 or idx >= len(children):\n",
                "            raise IndexError(f\"無効なパスインデックス {idx}。子要素の長さは {len(children)} です。\")\n",
                "        current = children[idx]\n",
                "        if not isinstance(current, dict):\n",
                "            raise ValueError(\"無効な AST: ノードはオブジェクトである必要があります。\")\n",
                "        t = current.get(\"section_title\")\n",
                "        titles.append(str(t) if t is not None else \"\")\n",
                "    return titles\n",
                "\n",
                "\n",
                "def _find_nodes_by_title(\n",
                "    ast: Dict[str, Any],\n",
                "    title_query: str,\n",
                "    *,\n",
                "    max_results: int,\n",
                "    case_sensitive: bool,\n",
                ") -> List[Dict[str, Any]]:\n",
                "    if not title_query:\n",
                "        return []\n",
                "\n",
                "    q = title_query if case_sensitive else title_query.lower()\n",
                "    results: List[Dict[str, Any]] = []\n",
                "\n",
                "    def walk(node: Dict[str, Any], path: List[int]) -> None:\n",
                "        if len(results) >= max_results:\n",
                "            return\n",
                "        title = node.get(\"section_title\") or \"\"\n",
                "        hay = title if case_sensitive else str(title).lower()\n",
                "        if q in hay:\n",
                "            results.append({\"path\": path, \"section_title\": node.get(\"section_title\")})\n",
                "            if len(results) >= max_results:\n",
                "                return\n",
                "\n",
                "        for i, child in enumerate(_get_children_list(node)):\n",
                "            if not isinstance(child, dict):\n",
                "                continue\n",
                "            walk(child, path + [i])\n",
                "\n",
                "    root = ast.get(\"root\")\n",
                "    if isinstance(root, dict):\n",
                "        walk(root, [])\n",
                "    return results\n",
                "\n",
                "\n",
                "class ASTStoreArgs(BaseModel):\n",
                "    action: Literal[\n",
                "        # read-only\n",
                "        \"load\",\n",
                "        \"load_subtree\",\n",
                "        \"load_meta\",\n",
                "        \"find_by_title\",\n",
                "        \"list_children\",\n",
                "        \"resolve_path\",\n",
                "        # write\n",
                "        \"init\",\n",
                "        \"ensure_path\",\n",
                "        \"append_child\",\n",
                "        \"append_child_by_titles\",\n",
                "        \"upsert_child_by_title\",\n",
                "        \"upsert_child_by_titles\",\n",
                "        \"update_node\",\n",
                "        \"update_node_by_titles\",\n",
                "        \"append_to_summary\",\n",
                "        \"append_to_summary_by_titles\",\n",
                "    ] = Field(..., description=\"永続化された AST に対して実行する操作。\")\n",
                "\n",
                "    ast_path: str = Field(\n",
                "        \"ast_state.json\",\n",
                "        description=\"AST JSON ファイルのパス（作成または更新される）。\",\n",
                "    )\n",
                "\n",
                "    # init\n",
                "    file_name: Optional[str] = Field(None, description=\"AST の文書名（init に必要）。\")\n",
                "    root_title: Optional[str] = Field(None, description=\"ルートノードのタイトル（オプション）。\")\n",
                "    root_summary: Optional[str] = Field(\"\", description=\"ルートノードの content_summary。\")\n",
                "\n",
                "    # navigation (index)\n",
                "    node_path: Optional[List[int]] = Field(None, description=\"ターゲットノードのパス（ルートからのインデックスリスト）。\")\n",
                "    parent_path: Optional[List[int]] = Field(None, description=\"親ノードのパス（ルートからのインデックスリスト）。\")\n",
                "\n",
                "    # navigation (titles)\n",
                "    node_titles: Optional[List[str]] = Field(None, description=\"ターゲットノードのタイトルパス（ルートから）。\")\n",
                "    parent_titles: Optional[List[str]] = Field(None, description=\"親ノードのタイトルパス（ルートから）。\")\n",
                "\n",
                "    # node data\n",
                "    section_title: Optional[str] = Field(None, description=\"新しいノードまたは更新されるノードのセクションタイトル。\")\n",
                "    content_summary: Optional[str] = Field(None, description=\"新しいノードまたは更新されるノードのセクションコンテンツ要約。\")\n",
                "    append_text: Optional[str] = Field(None, description=\"content_summary に追加するテキスト。\")\n",
                "\n",
                "    # append options\n",
                "    position: Optional[int] = Field(None, description=\"親の子要素の下での挿入位置。省略すると末尾に追加される。\")\n",
                "\n",
                "    # ensure/resolve options\n",
                "    create_missing: bool = Field(False, description=\"true の場合、ensure_path で不足しているノードを作成する。\")\n",
                "    created_default_summary: str = Field(\"\", description=\"自動作成されたノードのデフォルト要約。\")\n",
                "\n",
                "    # find options\n",
                "    title_query: Optional[str] = Field(None, description=\"section_title 内で検索する部分文字列。\")\n",
                "    max_results: int = Field(20, description=\"検索操作での最大一致数。\")\n",
                "    case_sensitive: bool = Field(False, description=\"タイトルマッチングが大文字小文字を区別するかどうか。\")\n",
                "\n",
                "    # edit guard\n",
                "    purpose: Optional[Literal[\n",
                "        \"append_child\",\n",
                "        \"upsert_child\",\n",
                "        \"update_node\",\n",
                "        \"append_to_summary\",\n",
                "        \"ensure_path\",\n",
                "    ]] = Field(None, description=\"action=load_meta に必要。次に実行する書き込み操作を示す。\")\n",
                "\n",
                "    edit_token: Optional[str] = Field(None, description=\"load_meta によって返されるワンタイムトークン。書き込み操作に必要。\")\n",
                "\n",
                "    include_children: bool = Field(True, description=\"load_meta/list_children 用: 現在の子要素のタイトルとインデックスを含める。\")\n",
                "\n",
                "\n",
                "@tool(args_schema=ASTStoreArgs)\n",
                "def ast_store(\n",
                "    action: str,\n",
                "    ast_path: str = \"ast_state.json\",\n",
                "    file_name: Optional[str] = None,\n",
                "    root_title: Optional[str] = None,\n",
                "    root_summary: str = \"\",\n",
                "    # navigation (index)\n",
                "    node_path: Optional[List[int]] = None,\n",
                "    parent_path: Optional[List[int]] = None,\n",
                "    # navigation (titles)\n",
                "    node_titles: Optional[List[str]] = None,\n",
                "    parent_titles: Optional[List[str]] = None,\n",
                "    # node data\n",
                "    section_title: Optional[str] = None,\n",
                "    content_summary: Optional[str] = None,\n",
                "    append_text: Optional[str] = None,\n",
                "    # append options\n",
                "    position: Optional[int] = None,\n",
                "    # ensure/resolve options\n",
                "    create_missing: bool = False,\n",
                "    created_default_summary: str = \"\",\n",
                "    # find options\n",
                "    title_query: Optional[str] = None,\n",
                "    max_results: int = 20,\n",
                "    case_sensitive: bool = False,\n",
                "    # edit guard\n",
                "    purpose: Optional[str] = None,\n",
                "    edit_token: Optional[str] = None,\n",
                "    include_children: bool = True,\n",
                ") -> str:\n",
                "    \"\"\"永続化された AST エディタ。常にディスクから現在の AST を読み取り、すぐに書き戻す。\"\"\"\n",
                "    try:\n",
                "        node_path_n = _normalize_path_indices(node_path)\n",
                "        parent_path_n = _normalize_path_indices(parent_path)\n",
                "\n",
                "        # init\n",
                "        if action == \"init\":\n",
                "            if not file_name:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=init には file_name が必要です\"})\n",
                "\n",
                "            now = _utc_now_iso()\n",
                "            ast: Dict[str, Any] = {\n",
                "                \"file_name\": file_name,\n",
                "                \"__meta__\": {\"rev\": 0, \"updated_at\": now},\n",
                "                \"root\": _make_node(root_title or file_name, root_summary or \"\"),\n",
                "            }\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"init\",\n",
                "                    \"ast_path\": ast_path,\n",
                "                    \"file_name\": file_name,\n",
                "                    \"rev\": 0,\n",
                "                    \"updated_at\": now,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        # その他のアクションには既存のファイルが必要\n",
                "        if not os.path.exists(ast_path):\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": False,\n",
                "                    \"error\": f\"AST ファイルが見つかりません: {ast_path}。まず action=init を呼び出してください。\",\n",
                "                }\n",
                "            )\n",
                "\n",
                "        ast = _load_json(ast_path)\n",
                "        meta = _get_meta(ast)\n",
                "        current_rev = int(meta.get(\"rev\") or 0)\n",
                "\n",
                "        if action == \"load\":\n",
                "            return _dump_json({\"ok\": True, \"action\": \"load\", \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\"), \"ast\": ast})\n",
                "\n",
                "        if action == \"load_subtree\":\n",
                "            ref = _traverse(ast, node_path_n)\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"load_subtree\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"node_path\": node_path_n,\n",
                "                    \"node\": ref.node,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"resolve_path\":\n",
                "            if not node_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=resolve_path には node_titles が必要です\"})\n",
                "            path, _created = _ensure_titles_path(\n",
                "                ast,\n",
                "                node_titles,\n",
                "                create_missing=False,\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"resolve_path\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"node_titles\": node_titles,\n",
                "                    \"node_path\": path,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"list_children\":\n",
                "            if node_titles:\n",
                "                path, _created = _ensure_titles_path(\n",
                "                    ast,\n",
                "                    node_titles,\n",
                "                    create_missing=False,\n",
                "                    created_default_summary=created_default_summary,\n",
                "                )\n",
                "            else:\n",
                "                path = node_path_n\n",
                "            ref = _traverse(ast, path)\n",
                "            children = _get_children_list(ref.node)\n",
                "            children_info = []\n",
                "            if include_children:\n",
                "                for i, ch in enumerate(children):\n",
                "                    if isinstance(ch, dict):\n",
                "                        children_info.append({\"index\": i, \"section_title\": ch.get(\"section_title\")})\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"list_children\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"node_path\": path,\n",
                "                    \"node_titles\": _titles_for_path(ast, path),\n",
                "                    \"children\": children_info,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"load_meta\":\n",
                "            if not purpose:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=load_meta には purpose が必要です\"})\n",
                "\n",
                "            # スコープノードを解決（編集対象のノード）\n",
                "            if node_titles:\n",
                "                scope_path, _created = _ensure_titles_path(\n",
                "                    ast,\n",
                "                    node_titles,\n",
                "                    create_missing=False,\n",
                "                    created_default_summary=created_default_summary,\n",
                "                )\n",
                "            else:\n",
                "                scope_path = node_path_n\n",
                "\n",
                "            scope_ref = _traverse(ast, scope_path)\n",
                "            children_info = []\n",
                "            if include_children:\n",
                "                for i, ch in enumerate(_get_children_list(scope_ref.node)):\n",
                "                    if isinstance(ch, dict):\n",
                "                        children_info.append({\"index\": i, \"section_title\": ch.get(\"section_title\")})\n",
                "\n",
                "            token = _issue_edit_token(\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=str(purpose),\n",
                "                scope_value={\"node_path\": scope_path},\n",
                "                issued_rev=current_rev,\n",
                "            )\n",
                "\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"load_meta\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"purpose\": purpose,\n",
                "                    \"node_path\": scope_path,\n",
                "                    \"node_titles\": _titles_for_path(ast, scope_path),\n",
                "                    \"children\": children_info,\n",
                "                    \"edit_token\": token,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"find_by_title\":\n",
                "            q = title_query or \"\"\n",
                "            matches = _find_nodes_by_title(\n",
                "                ast,\n",
                "                q,\n",
                "                max_results=max(1, int(max_results)),\n",
                "                case_sensitive=bool(case_sensitive),\n",
                "            )\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"find_by_title\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"title_query\": q,\n",
                "                    \"matches\": matches,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        # --- 書き込みアクション（load_meta によって発行された edit_token が必要） ---\n",
                "        if action == \"ensure_path\":\n",
                "            if not node_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=ensure_path には node_titles が必要です\", \"rev\": current_rev})\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"ensure_path\",\n",
                "                scope_value={\"node_path\": []},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            path, created_any = _ensure_titles_path(\n",
                "                ast,\n",
                "                node_titles,\n",
                "                create_missing=bool(create_missing),\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "\n",
                "            if created_any:\n",
                "                new_meta = _bump_meta(ast)\n",
                "                _atomic_write_text(ast_path, _dump_json(ast))\n",
                "                return _dump_json(\n",
                "                    {\n",
                "                        \"ok\": True,\n",
                "                        \"action\": \"ensure_path\",\n",
                "                        \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                        \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                        \"node_titles\": node_titles,\n",
                "                        \"node_path\": path,\n",
                "                        \"created\": True,\n",
                "                    }\n",
                "                )\n",
                "\n",
                "            # no change\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"ensure_path\",\n",
                "                    \"rev\": current_rev,\n",
                "                    \"updated_at\": meta.get(\"updated_at\"),\n",
                "                    \"node_titles\": node_titles,\n",
                "                    \"node_path\": path,\n",
                "                    \"created\": False,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"append_child_by_titles\":\n",
                "            if content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=append_child_by_titles には content_summary が必要です\", \"rev\": current_rev})\n",
                "            if not parent_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=append_child_by_titles には parent_titles が必要です\", \"rev\": current_rev})\n",
                "\n",
                "            parent_path_resolved, _created = _ensure_titles_path(\n",
                "                ast,\n",
                "                parent_titles,\n",
                "                create_missing=False,\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"append_child\",\n",
                "                scope_value={\"node_path\": parent_path_resolved},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            parent_ref = _traverse(ast, parent_path_resolved)\n",
                "            children = _get_children_list(parent_ref.node)\n",
                "\n",
                "            new_node = _make_node(section_title, content_summary)\n",
                "            if position is None:\n",
                "                children.append(new_node)\n",
                "                new_index = len(children) - 1\n",
                "            else:\n",
                "                pos = int(position)\n",
                "                if pos < 0 or pos > len(children):\n",
                "                    return _dump_json({\"ok\": False, \"error\": f\"position out of range: {pos} (0..{len(children)})\", \"rev\": current_rev})\n",
                "                children.insert(pos, new_node)\n",
                "                new_index = pos\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"append_child_by_titles\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"parent_titles\": parent_titles,\n",
                "                    \"parent_path\": parent_path_resolved,\n",
                "                    \"new_node_path\": parent_path_resolved + [new_index],\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"upsert_child_by_titles\":\n",
                "            if not section_title:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=upsert_child_by_titles には section_title が必要です\", \"rev\": current_rev})\n",
                "            if content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=upsert_child_by_titles には content_summary が必要です\", \"rev\": current_rev})\n",
                "            if not parent_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=upsert_child_by_titles には parent_titles が必要です\", \"rev\": current_rev})\n",
                "\n",
                "            parent_path_resolved, _created = _ensure_titles_path(\n",
                "                ast,\n",
                "                parent_titles,\n",
                "                create_missing=False,\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"upsert_child\",\n",
                "                scope_value={\"node_path\": parent_path_resolved},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            parent_ref = _traverse(ast, parent_path_resolved)\n",
                "            children = _get_children_list(parent_ref.node)\n",
                "\n",
                "            found_index: Optional[int] = None\n",
                "            target_norm = _normalize_title(section_title)\n",
                "            for i, child in enumerate(children):\n",
                "                if not isinstance(child, dict):\n",
                "                    continue\n",
                "                if _normalize_title(child.get(\"section_title\")) == target_norm:\n",
                "                    found_index = i\n",
                "                    break\n",
                "\n",
                "            if found_index is None:\n",
                "                children.append(_make_node(section_title, content_summary))\n",
                "                found_index = len(children) - 1\n",
                "                op = \"created\"\n",
                "            else:\n",
                "                child = children[found_index]\n",
                "                existing = str(child.get(\"content_summary\") or \"\")\n",
                "                if existing:\n",
                "                    child[\"content_summary\"] = existing.rstrip() + \"\\n\" + str(content_summary).lstrip()\n",
                "                else:\n",
                "                    child[\"content_summary\"] = str(content_summary)\n",
                "                op = \"appended\"\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"upsert_child_by_titles\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"parent_titles\": parent_titles,\n",
                "                    \"parent_path\": parent_path_resolved,\n",
                "                    \"node_path\": parent_path_resolved + [found_index],\n",
                "                    \"op\": op,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"update_node_by_titles\":\n",
                "            if not node_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=update_node_by_titles には node_titles が必要です\", \"rev\": current_rev})\n",
                "            if section_title is None and content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=update_node_by_titles には section_title または content_summary のいずれかが必要です\", \"rev\": current_rev})\n",
                "\n",
                "            node_path_resolved, _created = _ensure_titles_path(\n",
                "                ast,\n",
                "                node_titles,\n",
                "                create_missing=False,\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"update_node\",\n",
                "                scope_value={\"node_path\": node_path_resolved},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            ref = _traverse(ast, node_path_resolved)\n",
                "            if section_title is not None:\n",
                "                ref.node[\"section_title\"] = section_title\n",
                "            if content_summary is not None:\n",
                "                ref.node[\"content_summary\"] = content_summary\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"update_node_by_titles\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"node_titles\": node_titles,\n",
                "                    \"node_path\": node_path_resolved,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"append_to_summary_by_titles\":\n",
                "            if not node_titles:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=append_to_summary_by_titles には node_titles が必要です\", \"rev\": current_rev})\n",
                "            if append_text is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"action=append_to_summary_by_titles には append_text が必要です\", \"rev\": current_rev})\n",
                "\n",
                "            node_path_resolved, _created = _ensure_titles_path(\n",
                "                ast,\n",
                "                node_titles,\n",
                "                create_missing=False,\n",
                "                created_default_summary=created_default_summary,\n",
                "            )\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"append_to_summary\",\n",
                "                scope_value={\"node_path\": node_path_resolved},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            ref = _traverse(ast, node_path_resolved)\n",
                "            existing = str(ref.node.get(\"content_summary\") or \"\")\n",
                "            if existing:\n",
                "                ref.node[\"content_summary\"] = existing.rstrip() + \"\\n\" + str(append_text).lstrip()\n",
                "            else:\n",
                "                ref.node[\"content_summary\"] = str(append_text)\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"append_to_summary_by_titles\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"node_titles\": node_titles,\n",
                "                    \"node_path\": node_path_resolved,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        # --- legacy index-based write actions (also token-guarded) ---\n",
                "        if action == \"append_child\":\n",
                "            if content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"content_summary is required for action=append_child\", \"rev\": current_rev})\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"append_child\",\n",
                "                scope_value={\"node_path\": parent_path_n},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            parent_ref = _traverse(ast, parent_path_n)\n",
                "            children = _get_children_list(parent_ref.node)\n",
                "\n",
                "            new_node = _make_node(section_title, content_summary)\n",
                "            if position is None:\n",
                "                children.append(new_node)\n",
                "                new_index = len(children) - 1\n",
                "            else:\n",
                "                pos = int(position)\n",
                "                if pos < 0 or pos > len(children):\n",
                "                    return _dump_json({\"ok\": False, \"error\": f\"position out of range: {pos} (0..{len(children)})\", \"rev\": current_rev})\n",
                "                children.insert(pos, new_node)\n",
                "                new_index = pos\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"append_child\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"parent_path\": parent_path_n,\n",
                "                    \"new_node_path\": parent_path_n + [new_index],\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"upsert_child_by_title\":\n",
                "            if not section_title:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"section_title is required for action=upsert_child_by_title\", \"rev\": current_rev})\n",
                "            if content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"content_summary is required for action=upsert_child_by_title\", \"rev\": current_rev})\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"upsert_child\",\n",
                "                scope_value={\"node_path\": parent_path_n},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            parent_ref = _traverse(ast, parent_path_n)\n",
                "            children = _get_children_list(parent_ref.node)\n",
                "\n",
                "            found_index: Optional[int] = None\n",
                "            target_norm = _normalize_title(section_title)\n",
                "            for i, child in enumerate(children):\n",
                "                if not isinstance(child, dict):\n",
                "                    continue\n",
                "                if _normalize_title(child.get(\"section_title\")) == target_norm:\n",
                "                    found_index = i\n",
                "                    break\n",
                "\n",
                "            if found_index is None:\n",
                "                children.append(_make_node(section_title, content_summary))\n",
                "                found_index = len(children) - 1\n",
                "                op = \"created\"\n",
                "            else:\n",
                "                child = children[found_index]\n",
                "                existing = str(child.get(\"content_summary\") or \"\")\n",
                "                if existing:\n",
                "                    child[\"content_summary\"] = existing.rstrip() + \"\\n\" + str(content_summary).lstrip()\n",
                "                else:\n",
                "                    child[\"content_summary\"] = str(content_summary)\n",
                "                op = \"appended\"\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"upsert_child_by_title\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"parent_path\": parent_path_n,\n",
                "                    \"node_path\": parent_path_n + [found_index],\n",
                "                    \"op\": op,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"update_node\":\n",
                "            if section_title is None and content_summary is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"section_title and/or content_summary must be provided for action=update_node\", \"rev\": current_rev})\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"update_node\",\n",
                "                scope_value={\"node_path\": node_path_n},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            ref = _traverse(ast, node_path_n)\n",
                "            if section_title is not None:\n",
                "                ref.node[\"section_title\"] = section_title\n",
                "            if content_summary is not None:\n",
                "                ref.node[\"content_summary\"] = content_summary\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"update_node\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"node_path\": node_path_n,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        if action == \"append_to_summary\":\n",
                "            if append_text is None:\n",
                "                return _dump_json({\"ok\": False, \"error\": \"append_text is required for action=append_to_summary\", \"rev\": current_rev})\n",
                "\n",
                "            token_err = _consume_edit_token(\n",
                "                token=edit_token or \"\",\n",
                "                ast_path=ast_path,\n",
                "                scope_kind=\"append_to_summary\",\n",
                "                scope_value={\"node_path\": node_path_n},\n",
                "                current_rev=current_rev,\n",
                "            )\n",
                "            if token_err:\n",
                "                return _dump_json({\"ok\": False, \"error\": token_err, \"rev\": current_rev, \"updated_at\": meta.get(\"updated_at\")})\n",
                "\n",
                "            ref = _traverse(ast, node_path_n)\n",
                "            existing = str(ref.node.get(\"content_summary\") or \"\")\n",
                "            if existing:\n",
                "                ref.node[\"content_summary\"] = existing.rstrip() + \"\\n\" + str(append_text).lstrip()\n",
                "            else:\n",
                "                ref.node[\"content_summary\"] = str(append_text)\n",
                "\n",
                "            new_meta = _bump_meta(ast)\n",
                "            _atomic_write_text(ast_path, _dump_json(ast))\n",
                "            return _dump_json(\n",
                "                {\n",
                "                    \"ok\": True,\n",
                "                    \"action\": \"append_to_summary\",\n",
                "                    \"rev\": int(new_meta.get(\"rev\") or 0),\n",
                "                    \"updated_at\": new_meta.get(\"updated_at\"),\n",
                "                    \"node_path\": node_path_n,\n",
                "                }\n",
                "            )\n",
                "\n",
                "        return _dump_json({\"ok\": False, \"error\": f\"Unknown action: {action}\"})\n",
                "\n",
                "    except Exception as e:\n",
                "        return _dump_json({\"ok\": False, \"error\": str(e)})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 構造化出力（スキーマ）の定義"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 130,
            "metadata": {},
            "outputs": [],
            "source": [
                "# class DocumentNode(BaseModel):\n",
                "#     section_title: Optional[str] = Field(None, description=\"セクションのタイトル\")\n",
                "#     content_summary: str = Field(..., description=\"このセクションのコンテンツの簡潔な要約\")\n",
                "#     children: List['DocumentNode'] = Field(default_factory=list, description=\"サブセクションまたはネストされたコンテンツ\")\n",
                "\n",
                "\n",
                "# class DocumentAST(BaseModel):\n",
                "#     file_name: str = Field(..., description=\"解析されたファイル名\")\n",
                "#     root: DocumentNode = Field(..., description=\"文書構造のルートノード\")\n",
                "\n",
                "\n",
                "# class AgentResult(BaseModel):\n",
                "#     status: str = Field(..., description=\"実行ステータス（例: 'ok'）\")\n",
                "#     ast_path: str = Field(..., description=\"永続化された AST JSON のパス\")\n",
                "#     note: Optional[str] = Field(None, description=\"オプションのメッセージ\")\n",
                "\n",
                "\n",
                "# # pydantic v2: resolve forward references\n",
                "# DocumentNode.model_rebuild()\n",
                "# DocumentAST.model_rebuild()\n",
                "# AgentResult.model_rebuild()\n",
                "\n",
                "\n",
                "class ValidationRules(BaseModel):\n",
                "    \"\"\"\n",
                "    見出し抽出時の誤検知を防ぐための検証ルールセット。\n",
                "    正規表現マッチ後の追加チェックに使用します。\n",
                "    \"\"\"\n",
                "    max_length: Optional[int] = Field(\n",
                "        None, \n",
                "        description=\"見出しとして許容される最大文字数。これを超えると本文とみなす。\"\n",
                "    )\n",
                "    requires_line_start: Optional[bool] = Field(\n",
                "        True, \n",
                "        description=\"正規表現が行頭(^)からマッチする必要があるか。\"\n",
                "    )\n",
                "    requires_prev_empty_line: Optional[bool] = Field(\n",
                "        None, \n",
                "        description=\"直前の行が空行であることを必須とするか。\"\n",
                "    )\n",
                "    must_contain: Optional[List[str]] = Field(\n",
                "        None, \n",
                "        description=\"見出し文字列に含まれていなければならない特定の文字リスト（例: ['【', '】']）。\"\n",
                "    )\n",
                "    must_not_contain: Optional[List[str]] = Field(\n",
                "        None, \n",
                "        description=\"見出し文字列に含まれていてはいけない文字リスト。\"\n",
                "    )\n",
                "    must_not_end_with: Optional[List[str]] = Field(\n",
                "        None, \n",
                "        description=\"行末に来てはいけない文字リスト（例: ['。', '.']）。\"\n",
                "    )\n",
                "    context_filters: Optional[List[str]] = Field(\n",
                "        None, \n",
                "        description=\"自然言語で記述された文脈的な判定ロジック。Workerロジックで実装が必要な複雑な条件。\"\n",
                "    )\n",
                "\n",
                "class HierarchyRule(BaseModel):\n",
                "    \"\"\"\n",
                "    文書の階層構造（Level）ごとの定義ルール。\n",
                "    \"\"\"\n",
                "    level: int = Field(..., description=\"階層レベル（0: メタ情報, 1: 最上位, ...）\")\n",
                "    name: str = Field(..., description=\"階層の名称（識別子）\")\n",
                "    regex: str = Field(..., description=\"この階層を抽出するためのPython正規表現文字列\")\n",
                "    parent_level: Optional[int] = Field(\n",
                "        None, \n",
                "        description=\"親となる階層レベル。Noneの場合はルート要素（またはそれに準ずる）。\"\n",
                "    )\n",
                "    description: Optional[str] = Field(None, description=\"ルールの説明・メモ\")\n",
                "    validation_rules: Optional[ValidationRules] = Field(\n",
                "        default_factory=ValidationRules,\n",
                "        description=\"正規表現マッチ後に適用するバリデーションルール\"\n",
                "    )\n",
                "\n",
                "    @field_validator('regex')\n",
                "    @classmethod\n",
                "    def validate_regex(cls, v: str) -> str:\n",
                "        \"\"\"提供された正規表現文字列がPythonでコンパイル可能かチェックする\"\"\"\n",
                "        try:\n",
                "            re.compile(v)\n",
                "        except re.error as e:\n",
                "            raise ValueError(f\"Invalid regex pattern: {v}. Error: {e}\")\n",
                "        return v\n",
                "\n",
                "class ExclusionRule(BaseModel):\n",
                "    \"\"\"\n",
                "    最初から除外すべき行（ページ番号や特定の箇条書きなど）のルール。\n",
                "    \"\"\"\n",
                "    regex: str = Field(..., description=\"除外対象とする行の正規表現\")\n",
                "    description: Optional[str] = Field(None, description=\"除外理由の説明\")\n",
                "\n",
                "    @field_validator('regex')\n",
                "    @classmethod\n",
                "    def validate_regex(cls, v: str) -> str:\n",
                "        try:\n",
                "            re.compile(v)\n",
                "        except re.error as e:\n",
                "            raise ValueError(f\"Invalid regex pattern: {v}. Error: {e}\")\n",
                "        return v\n",
                "\n",
                "class DocumentStructureBlueprint(BaseModel):\n",
                "    \"\"\"\n",
                "    エージェントが生成するドキュメント構造定義書のルートモデル。\n",
                "    \"\"\"\n",
                "    hierarchy_structure: List[HierarchyRule] = Field(\n",
                "        ..., \n",
                "        description=\"階層定義のリスト。レベル順にソートされていることが望ましい。\"\n",
                "    )\n",
                "    global_exclusion_rules: Optional[Dict[str, ExclusionRule]] = Field(\n",
                "        default=None,\n",
                "        description=\"文書全体で適用される除外ルール（キーはルール名）\"\n",
                "    )\n",
                "\n",
                "    def get_rule_by_level(self, level: int) -> Optional[HierarchyRule]:\n",
                "        \"\"\"指定されたレベルのルールを取得するヘルパーメソッド\"\"\"\n",
                "        for rule in self.hierarchy_structure:\n",
                "            if rule.level == level:\n",
                "                return rule\n",
                "        return None\n",
                "\n",
                "tools = [read_text_segment, read_text_file, extract_regex_matches, get_file_length] # , ast_store\n",
                "middleware = [\n",
                "    SummarizationMiddleware(\n",
                "            model=\"gpt-5.2\",\n",
                "            trigger=(\"tokens\", 10000),\n",
                "            keep=(\"messages\", 5),\n",
                "        ),\n",
                "    TodoListMiddleware()\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. エージェントの設定"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 未作成：AST"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM (OpenAI or Azure OpenAI)\n",
                "llm = build_llm()\n",
                "\n",
                "system_prompt = \"\"\"\n",
                "あなたは高度な文書解析エージェントです。\n",
                "目標は、テキストファイルを解析し、その構造を抽象構文木（AST）として再構築することです。\n",
                "\n",
                "重要な注意事項:\n",
                "- チャットの記憶に AST 全体を保持することに依存してはいけません。\n",
                "- ast_store ツールを使用して、AST をディスクに段階的に永続化してください。\n",
                "\n",
                "ワークフロー（重要: 書き込み操作はトークンガード付きです）:\n",
                "\n",
                "計画 / タスクリスト（必須）:\n",
                "- この実行の開始時に、write_todos を呼び出してタスク計画（2-8項目）を作成してください。\n",
                "- 実行中は、タスクリストを正確に保ってください: 1つの項目を in_progress にマークし、完了したら completed にマークして、次の項目に進んでください。\n",
                "- 新しい必要な作業を発見した場合は、タスクリストに追加し、すべてのタスクが完了するまで追跡を続けてください。\n",
                "\n",
                "AST の初期化:\n",
                "- ast_store(action=\"load\", ast_path=...) を使用して AST ファイルが存在するか確認してください。\n",
                "- AST が空または存在しない場合は、ast_store(action=\"init\", file_name=..., root_title=..., root_summary=...) を使用して初期化してください。\n",
                "- その後、自律的に文書をスキャンして、すべての見出しパターンを検出し、完全な AST 構造を構築してください。\n",
                "- 書き込み（追加/更新）の前に、必ず以下を呼び出してください:\n",
                "  - ast_store(action=\"load_meta\", ast_path=..., purpose=..., node_titles=[...])\n",
                "  これは、その特定のノードと現在のリビジョンにスコープされたワンタイム edit_token を返します。\n",
                "- その後、その edit_token を使用して正確に1回の書き込みを実行してください。\n",
                "  （別の書き込みが必要な場合は、load_meta を再度呼び出してください。）\n",
                "\n",
                "タイトルパス操作を優先（インデックスの誤カウントを避ける）:\n",
                "- 既存の親の下に新しい子を追加:\n",
                "  - load_meta: ast_store(action=\"load_meta\", purpose=\"append_child\", node_titles=[\"第一部【企業情報】\"])  # 親\n",
                "  - write: ast_store(action=\"append_child_by_titles\", parent_titles=[\"第一部【企業情報】\"], section_title=\"第５【…】\", content_summary=\"...\", edit_token=\"...\")\n",
                "- 子をアップサート（同じタイトルが親の下に存在する場合は要約に追加）:\n",
                "  - load_meta: ast_store(action=\"load_meta\", purpose=\"upsert_child\", node_titles=[\"第一部【企業情報】\",\"第１【企業の概況】\"])  # 親\n",
                "  - write: ast_store(action=\"upsert_child_by_titles\", parent_titles=[...], section_title=\"１【…】\", content_summary=\"...\", edit_token=\"...\")\n",
                "- 既存のノードを更新:\n",
                "  - load_meta: ast_store(action=\"load_meta\", purpose=\"update_node\", node_titles=[\"第一部【企業情報】\",\"第１【企業の概況】\",\"１【…】\"])  # ノード\n",
                "  - write: ast_store(action=\"update_node_by_titles\", node_titles=[...], content_summary=\"...\", edit_token=\"...\")\n",
                "- 既存のノードの要約に追加:\n",
                "  - load_meta: ast_store(action=\"load_meta\", purpose=\"append_to_summary\", node_titles=[...])\n",
                "  - write: ast_store(action=\"append_to_summary_by_titles\", node_titles=[...], append_text=\"...\", edit_token=\"...\")\n",
                "\n",
                "現在の状態を確認（読み取り専用）:\n",
                "- ast_store(action=\"list_children\", node_titles=[...])\n",
                "- ast_store(action=\"load_subtree\", node_path=[...])\n",
                "- ast_store(action=\"find_by_title\", title_query=\"...\")\n",
                "- ast_store(action=\"resolve_path\", node_titles=[...])\n",
                "\n",
                "ガイドライン:\n",
                "- 文書内の見出しパターンを自律的に検出してください。特定の形式（EDINET、Markdown など）を想定しないでください。\n",
                "- read_text_file を使用してファイルをチャンクで読み取り、見出しパターンを特定してください:\n",
                "  * 一般的なパターンには以下が含まれます: Markdown (#, ##, ###)、番号付きセクション (1., 1.1, 1.1.1)、括弧付き見出し（【...】）、章タイトル（第X章、Chapter X）など。\n",
                "  * 視覚的な指標を探してください: 目立つ行、繰り返しパターン、インデントなど。\n",
                "  * 文書固有の規則を考慮してください（例: \"独立監査人の監査報告書\" は標準的なフォーマットがなくても主要セクションである可能性があります）。\n",
                "- 検出された見出しに基づいて AST 階層を構築し、適切な親子関係を維持してください。\n",
                "- 各見出しの下のコンテンツを簡潔に要約してください。\n",
                "- AST が存在しない場合は、まず ast_store(action=\"init\", file_name=..., root_title=..., root_summary=...) を使用して初期化してください。\n",
                "\n",
                "ツールの使用方法:\n",
                "- read_text_file は、デフォルトで 100 文字のみを読み取ります（start と length が省略された場合）。これは、コンテキストウィンドウの制限を超えることを避けるためです。\n",
                "- 大きなファイルの特定のセグメントを読み取るには、start と length パラメータを使用して read_text_file を使用してください。\n",
                "- ファイルの特定の部分を読み取る必要がある場合にのみ、start と length パラメータを含めてください。\n",
                "- まず get_file_length を使用してファイルサイズを決定し、必要に応じてチャンクで読み取ってください。\n",
                "\n",
                "最終出力:\n",
                "- status=\"ok\" と提供された ast_path で AgentResult を返してください。\n",
                "- 最終応答に AST 全体を出力しないでください。\n",
                "\"\"\".strip()\n",
                "\n",
                "agent = create_agent(\n",
                "    model=llm,\n",
                "    tools=tools,\n",
                "    system_prompt=system_prompt,\n",
                "    response_format=AgentResult,\n",
                "    middleware=middleware,\n",
                "    debug=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 構造解析"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM (OpenAI or Azure OpenAI)\n",
                "llm = build_llm()\n",
                "\n",
                "# system_prompt = \"\"\"\n",
                "# あなたは高度な文書解析エージェントです。\n",
                "# 目標は、テキストファイルを解析し、その見出しの命名規則を抽出するための正規表現を生成することです。\n",
                "# 以下の手順で進めてください。\n",
                "# - TodoListMiddleware を使用して、詳細かつ具体的にタスクを管理してください。\n",
                "# - 以下のように進めてください。\n",
                "#   - ファイルの読み込み（先頭のみでなく、見出しの命名規則の仮説がある程度できるまで、読み込みを行ってください。）\n",
                "#   - 仮説のタスク化（TodoListMiddlewareで各タスクの検証をタスクリストに追加してください。）\n",
                "#   - 仮説の検証（正規表現で抽出したりキーワード検索をすることで仮説を検証してください。）\n",
                "#   - 仮説の修正・追加（検証結果に応じて仮説を修正や追加をしてください。）\n",
                "#   - 正規表現の出力（最終的な正規表現を出力してください。）\n",
                "\n",
                "# **注意事項**\n",
                "# - 命名規則は複数存在します。大項目、中項目、小項目、というように階層化されていることが多く、階層ごとに命名規則が異なることが多いです。\n",
                "# - 推測はせず、必ず対象の文書から事実を確認したうえで命名規則を生成してください。\n",
                "# \"\"\".strip()\n",
                "\n",
                "system_prompt = \"\"\"\n",
                "あなたは、未知のドキュメントの構造を解明する「ドキュメント構造設計アーキテクト」です。\n",
                "あなたの目標は、提供されたテキストファイルを分析し、後続のWorkerエージェントが正確にAST（抽象構文木）を構築できるように、**「階層構造の定義」**と**「各見出しの抽出ルール」**を確立することです。\n",
                "\n",
                "以下の手順で自律的に調査・設計を行ってください。\n",
                "全てのフェーズのタスクはTodoListMiddleware を使用して詳細かつ具体的にタスクを管理してください。\n",
                "\n",
                "## 1. 調査フェーズ (Sampling & Hypothesizing)\n",
                "- **サンプリング**: ファイル内を数か所読み込んでください。\n",
                "- **パターン発見**: 以下の視点でテキストの規則性を探してください。\n",
                "    - **記号パターン**: `第1章`, `1.`, `(1)`, `[A]`, `■` などの定型パターン。\n",
                "    - **インデント/空白**: 行頭の空白数や、空行の有無。\n",
                "    - **テキストの特徴**: 特定の接尾辞（「〜について」等）や、文字種の統一（全て全角等）。\n",
                "- **追加サンプリング**: 不十分と思われる場合は追加でファイル内を数か所読み込んでください。\n",
                "\n",
                "## 2. 検証フェーズ (Validation & Noise Filtering)\n",
                "- **仮説検証**: 作成した正規表現が、本文中の「単なる箇条書き」や「文中参照」を誤検知しないか、実際にテキスト検索を行って確認してください。\n",
                "- **文脈条件の定義**: 正規表現だけでは区別できない場合、以下のような「周辺条件（Context）」を定義してください。\n",
                "    - 「前の行が空行であること」\n",
                "    - 「文字数がN文字以内であること」\n",
                "    - 「行末が句点（。）で終わっていないこと」\n",
                "    - 「直後にインデントされた行が続くこと」\n",
                "\n",
                "## 3. 構造化フェーズ (Hierarchy Mapping)\n",
                "- 抽出したパターンに「階層レベル（Level 1, 2, 3...）」を割り当ててください。\n",
                "- 親子関係のルール（例：「(1) の親は必ず 1. である」）を明確にしてください。\n",
                "\n",
                "## 4. 監査と修正フェーズ (Audit & Refine)\n",
                "- 作成した正規表現のセットが、文書全体に対して整合性が取れているかをツールを用いて統計的に検証し、必要に応じて修正します。\n",
                "    - 検証方法の例: regex_patternsツールの引数に作成した正規表現を全て渡し、以下の観点で評価してください。\n",
                "        - 【空白検証】どの見出しにもヒットしない区間（その区間のテキストを実際に読み込み、見出しの取りこぼしがないか確認してください。必要に応じて正規表現を修正してください。）\n",
                "        - 【衝突検証】同一行に対して複数の見出しにヒットしている箇所（その区間のテキストを実際に読み込み、想定外の衝突かどうか判定し、必要に応じて正規表現の修正をしてください。）\n",
                "        - 【シーケンス不整合検証】抽出された見出しに含まれるシーケンスが不自然に飛んでいる箇所（その区間のテキストを実際に読み込み、連番が不自然に飛んでいるかどうか判定し、必要に応じて正規表現の修正をしてください。）\n",
                "\n",
                "## 5. 出力フェーズ (Output Blueprint)\n",
                "最終的に、以下のJSONフォーマットに従って成果物を出力してください。\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"hierarchy_structure\": [\n",
                "    {\n",
                "      \"level\": 1,\n",
                "      \"name\": \"Major_Section\",\n",
                "      \"regex\": \"(?m)^第[0-9]+章.+\",\n",
                "      \"description\": \"章ごとの区切り。行頭の第N章にマッチ。\",\n",
                "      \"validation_rules\": {\n",
                "         \"max_length\": 100\n",
                "      }\n",
                "    },\n",
                "    {\n",
                "      \"level\": 2,\n",
                "      \"name\": \"Sub_Section\",\n",
                "      \"regex\": \"(?m)^[0-9]+\\\\..+\",\n",
                "      \"parent_level\": 1,\n",
                "      \"validation_rules\": {\n",
                "         \"requires_prev_empty_line\": true,\n",
                "         \"must_not_end_with\": [\"。\", \".\"]\n",
                "      }\n",
                "    }\n",
                "    // ... 他の階層\n",
                "  ]\n",
                "}\n",
                "\"\"\".strip()\n",
                "\n",
                "agent = create_agent(\n",
                "    model=llm,\n",
                "    tools=tools,\n",
                "    system_prompt=system_prompt,\n",
                "    response_format=DocumentStructureBlueprint,\n",
                "    middleware=middleware,\n",
                "    debug=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# target_file = \"sample.txt\"\n",
                "target_file = \"富士フィルム_有価証券報告書.pdf\"\n",
                "\n",
                "if target_file.endswith(\".pdf\"):\n",
                "    # pymupdf (fitz)を使用して日本語PDFを正しく読み込む\n",
                "    import fitz  # PyMuPDF\n",
                "    doc = fitz.open(target_file)\n",
                "    text_content = []\n",
                "    for page_num in range(len(doc)):\n",
                "        page = doc[page_num]\n",
                "        text_content.append(page.get_text())\n",
                "    doc.close()\n",
                "    \n",
                "    target_file = target_file.replace(\".pdf\", \".txt\")\n",
                "    with open(target_file, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(\"\\n\".join(text_content))\n",
                "\n",
                "# ASTは都度ファイルに永続化する（LLMの記憶に依存しない）\n",
                "ast_path = f\"{target_file}.ast.json\"\n",
                "\n",
                "# ASTファイルが存在しない場合は初期化（エージェントが自律的に見出しを検出して構築する）\n",
                "if not os.path.exists(ast_path):\n",
                "    import json\n",
                "    from datetime import datetime, timezone\n",
                "    root_title = target_file.replace(\".txt\", \"\").replace(\".pdf\", \"\")\n",
                "    ast_init = {\n",
                "        \"file_name\": os.path.basename(target_file),\n",
                "        \"__meta__\": {\"rev\": 0, \"updated_at\": datetime.now(timezone.utc).isoformat()},\n",
                "        \"root\": {\n",
                "            \"section_title\": root_title,\n",
                "            \"content_summary\": \"\",\n",
                "            \"children\": []\n",
                "        }\n",
                "    }\n",
                "    with open(ast_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        json.dump(ast_init, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "query = (\n",
                "    f\"ファイル '{target_file}' を解析してください。 \"\n",
                "    f\"文書内のすべての見出しパターンを自律的に検出して、抽出するための正規表現を生成してください。\"\n",
                ")\n",
                "# query = (\n",
                "#     f\"ファイル '{target_file}' を解析してください。 \"\n",
                "#     f\"文書内のすべての見出しパターンを自律的に検出して、完全な AST 構造を '{ast_path}' に構築してください。 \"\n",
                "#     f\"最後に、status='ok' と ast_path='{ast_path}' で AgentResult を返してください。\"\n",
                "# )\n",
                "# query = (\n",
                "#     f\"ファイル '{target_file}' を解析してください。 \"\n",
                "#     f\"文書内のすべての見出しパターンを自律的に検出してください（特定の形式を想定しないでください）。 \"\n",
                "#     f\"ast_store ツールを使用して、完全な AST 構造を '{ast_path}' に構築し、永続化してください。 \"\n",
                "#     \"重要: すべての書き込み操作はトークンガード付きです。各書き込みの前に、必ず \"\n",
                "#     \"ast_store(action='load_meta', purpose=..., node_titles=[...]) を呼び出してワンタイム edit_token を取得し、 \"\n",
                "#     \"その後、その edit_token を使用して正確に1回の書き込みを実行してください。 \"\n",
                "#     \"インデックスの誤カウントを避けるため、タイトルパス操作を優先してください: append_child_by_titles / upsert_child_by_titles / \"\n",
                "#     \"update_node_by_titles / append_to_summary_by_titles。 \"\n",
                "#     \"親を決定する前に、list_children/find_by_title/resolve_path を使用して現在の AST 状態を確認してください。 \"\n",
                "#     \"見出しパターンを特定するためにファイルをチャンクで読み取ってください - Markdown (#, ##)、番号付きセクション (1., 1.1)、 \"\n",
                "#     \"括弧付き見出し（【...】）、章タイトル、および文書構造のその他の視覚的指標を探してください。 \"\n",
                "#     \"最終応答に AST 全体を出力しようとしないでください。 \"\n",
                "#     f\"最後に、status='ok' と ast_path='{ast_path}' で AgentResult を返してください。\"\n",
                "# )\n",
                "inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
                "result = agent.invoke(inputs)\n",
                "\n",
                "agent_status = result.get(\"structured_response\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
                "\n",
                "# tool_call_id -> ToolMessage\n",
                "_tool_messages = {\n",
                "    m.tool_call_id: m for m in result[\"messages\"] if isinstance(m, ToolMessage)\n",
                "}\n",
                "\n",
                "all_logs = []\n",
                "for idx, m in enumerate(result[\"messages\"]):\n",
                "    if isinstance(m, HumanMessage):\n",
                "        all_logs.append({\n",
                "            \"type\": \"human_message\",\n",
                "            \"index\": idx,\n",
                "            \"content\": m.content,\n",
                "        })\n",
                "    elif isinstance(m, AIMessage):\n",
                "        # AIの思考プロセス（ツール呼び出しがない場合）\n",
                "        if not m.tool_calls:\n",
                "            all_logs.append({\n",
                "                \"type\": \"ai_thought\",\n",
                "                \"index\": idx,\n",
                "                \"content\": m.content,\n",
                "            })\n",
                "        # ツール呼び出しがある場合\n",
                "        else:\n",
                "            for tc in m.tool_calls:\n",
                "                tool_msg = _tool_messages.get(tc.get(\"id\"))\n",
                "                all_logs.append({\n",
                "                    \"type\": \"tool_call\",\n",
                "                    \"index\": idx,\n",
                "                    \"tool_name\": tc.get(\"name\"),\n",
                "                    \"args\": tc.get(\"args\"),\n",
                "                    \"tool_call_id\": tc.get(\"id\"),\n",
                "                    \"ai_content\": m.content,  # AIの思考プロセスも含める\n",
                "                    \"output\": getattr(tool_msg, \"content\", None) if tool_msg else None,\n",
                "                    \"status\": getattr(tool_msg, \"status\", None) if tool_msg else None,\n",
                "                })\n",
                "    elif isinstance(m, ToolMessage):\n",
                "        # ToolMessageは既にtool_callのログに含まれているので、必要に応じて追加\n",
                "        all_logs.append({\n",
                "            \"type\": \"tool_result\",\n",
                "            \"index\": idx,\n",
                "            \"tool_call_id\": m.tool_call_id,\n",
                "            # \"content\": m.content,\n",
                "            \"status\": getattr(m, \"status\", None),\n",
                "        })\n",
                "\n",
                "import json\n",
                "\n",
                "for log in all_logs:\n",
                "    print(json.dumps(log, ensure_ascii=False, indent=2))\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "if agent_status:\n",
                "    # Pydanticモデルの場合はdictに変換してJSON形式で表示\n",
                "    if hasattr(agent_status, 'model_dump'):\n",
                "        status_dict = agent_status.model_dump()\n",
                "    elif isinstance(agent_status, dict):\n",
                "        status_dict = agent_status\n",
                "    else:\n",
                "        status_dict = {\"raw\": str(agent_status)}\n",
                "    \n",
                "    print(json.dumps(status_dict, ensure_ascii=False, indent=2))\n",
                "else:\n",
                "    print(\"agent_status is None or empty\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
